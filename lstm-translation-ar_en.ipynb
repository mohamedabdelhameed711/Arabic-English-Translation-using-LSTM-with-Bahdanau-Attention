{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Required Libraries\n","metadata":{}},{"cell_type":"code","source":"! pip install sacremoses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:43:43.458657Z","iopub.execute_input":"2025-05-15T10:43:43.459227Z","iopub.status.idle":"2025-05-15T10:43:48.321907Z","shell.execute_reply.started":"2025-05-15T10:43:43.459205Z","shell.execute_reply":"2025-05-15T10:43:48.320926Z"}},"outputs":[{"name":"stdout","text":"Collecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.5.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.1.1\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport random\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom transformers import AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:12:59.473085Z","iopub.execute_input":"2025-05-15T11:12:59.473805Z","iopub.status.idle":"2025-05-15T11:12:59.477492Z","shell.execute_reply.started":"2025-05-15T11:12:59.473779Z","shell.execute_reply":"2025-05-15T11:12:59.476832Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset('Helsinki-NLP/tatoeba_mt', 'ara-eng', split='test[:5%]')\narabic_sentences = [item['sourceString'] for item in dataset]\nenglish_sentences = [item['targetString'] for item in dataset]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:13:00.055853Z","iopub.execute_input":"2025-05-15T11:13:00.056120Z","iopub.status.idle":"2025-05-15T11:13:02.816884Z","shell.execute_reply.started":"2025-05-15T11:13:00.056099Z","shell.execute_reply":"2025-05-15T11:13:02.816305Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ar-en\")\n\npairs = []\nfor ar, en in zip(arabic_sentences, english_sentences):\n    ar_ids = tokenizer.encode(ar, return_tensors='pt', padding=False, truncation=True)[0]\n    en_ids = tokenizer.encode(en, return_tensors='pt', padding=False, truncation=True)[0]\n    pairs.append((ar_ids, en_ids))\n\nmax_ar_len = max(len(p[0]) for p in pairs)\nmax_en_len = max(len(p[1]) for p in pairs)\n\n# Pad sequences\ndef pad_sequence(seq, max_len):\n    return torch.cat([seq, torch.tensor([tokenizer.pad_token_id] * (max_len - len(seq)))])\n\nX = torch.stack([pad_sequence(p[0], max_ar_len) for p in pairs])\nY = torch.stack([pad_sequence(p[1], max_en_len) for p in pairs])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:13:02.818193Z","iopub.execute_input":"2025-05-15T11:13:02.818526Z","iopub.status.idle":"2025-05-15T11:13:03.214177Z","shell.execute_reply.started":"2025-05-15T11:13:02.818509Z","shell.execute_reply":"2025-05-15T11:13:03.213418Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"## Model (LSTM + Attention)","metadata":{}},{"cell_type":"code","source":"class BahdanauAttention(nn.Module):\n    def __init__(self, hid_dim):\n        super().__init__()\n        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n        self.v = nn.Parameter(torch.rand(hid_dim))\n\n    def forward(self, hidden, encoder_outputs):\n        src_len = encoder_outputs.shape[0]\n        batch_size = encoder_outputs.shape[1]\n\n        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n        attention = torch.bmm(v, energy.permute(0, 2, 1)).squeeze(1)\n\n        return F.softmax(attention, dim=1)\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.lstm = nn.LSTM(emb_dim, hid_dim)\n\n    def forward(self, src):\n        src = src.long()  # <-- Ensure Long tensor for embedding\n        embedded = self.embedding(src).permute(1, 0, 2)\n        outputs, (hidden, cell) = self.lstm(embedded)\n        return outputs, (hidden, cell)\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim):\n        super().__init__()\n        self.output_dim = output_dim\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.attention = BahdanauAttention(hid_dim)\n        self.lstm = nn.LSTM(emb_dim + hid_dim, hid_dim)\n        self.fc_out = nn.Linear(hid_dim * 2 + emb_dim, output_dim)\n\n    def forward(self, input, hidden, cell, encoder_outputs):\n        input = input.long()  # <-- Ensure Long tensor for embedding\n        input = input.unsqueeze(0)\n        embedded = self.embedding(input)\n\n        attn_weights = self.attention(hidden[-1], encoder_outputs).unsqueeze(1)\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        context = torch.bmm(attn_weights, encoder_outputs).permute(1, 0, 2)\n\n        rnn_input = torch.cat((embedded, context), dim=2)\n        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n        output = output.squeeze(0)\n        context = context.squeeze(0)\n        embedded = embedded.squeeze(0)\n\n        prediction = self.fc_out(torch.cat((output, context, embedded), dim=1))\n        return prediction, hidden, cell\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = trg.shape[0]\n        trg_len = trg.shape[1]\n        trg_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n        encoder_outputs, (hidden, cell) = self.encoder(src)\n        input = trg[:, 0]\n\n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n            outputs[:, t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = trg[:, t] if teacher_force else top1\n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:13:03.215124Z","iopub.execute_input":"2025-05-15T11:13:03.215415Z","iopub.status.idle":"2025-05-15T11:13:03.227553Z","shell.execute_reply.started":"2025-05-15T11:13:03.215392Z","shell.execute_reply":"2025-05-15T11:13:03.226772Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"EMB_DIM = 256\nHID_DIM = 512\nBATCH_SIZE = 16\nEPOCHS = 30\nVOCAB_SIZE = tokenizer.vocab_size\nINPUT_DIM = OUTPUT_DIM = VOCAB_SIZE\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM)\ndec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM)\nmodel = Seq2Seq(enc, dec, device).to(device)\n\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\nval_size = int(0.1 * len(X))\ntrain_size = len(X) - val_size\ntrain_ds, val_ds = random_split(TensorDataset(X, Y), [train_size, val_size])\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n\ndef train():\n    model.train()\n    for epoch in range(EPOCHS):\n        total_loss = 0\n        for src, trg in train_loader:\n            src = src.long().to(device)  # <-- Ensure Long before embedding\n            trg = trg.long().to(device)\n            optimizer.zero_grad()\n            output = model(src, trg)\n            output = output[:, 1:].reshape(-1, output.shape[-1])\n            trg = trg[:, 1:].reshape(-1)\n            loss = criterion(output, trg)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n\ndef validate():\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for src, trg in val_loader:\n            src = src.long().to(device)  # <-- Ensure Long before embedding\n            trg = trg.long().to(device)\n            output = model(src, trg, 0)\n            output = output[:, 1:].reshape(-1, output.shape[-1])\n            trg = trg[:, 1:].reshape(-1)\n            loss = criterion(output, trg)\n            total_loss += loss.item()\n    print(f\"Validation Loss: {total_loss / len(val_loader):.4f}\")\n\ntrain()\nvalidate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:13:06.255255Z","iopub.execute_input":"2025-05-15T11:13:06.255553Z","iopub.status.idle":"2025-05-15T11:35:19.713476Z","shell.execute_reply.started":"2025-05-15T11:13:06.255531Z","shell.execute_reply":"2025-05-15T11:35:19.712800Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 6.6146\nEpoch 2, Loss: 4.8713\nEpoch 3, Loss: 4.5002\nEpoch 4, Loss: 4.2292\nEpoch 5, Loss: 4.0755\nEpoch 6, Loss: 3.8515\nEpoch 7, Loss: 3.6796\nEpoch 8, Loss: 3.5118\nEpoch 9, Loss: 3.2970\nEpoch 10, Loss: 2.9954\nEpoch 11, Loss: 2.8104\nEpoch 12, Loss: 2.4330\nEpoch 13, Loss: 2.1774\nEpoch 14, Loss: 1.9171\nEpoch 15, Loss: 1.5747\nEpoch 16, Loss: 1.3772\nEpoch 17, Loss: 1.0652\nEpoch 18, Loss: 0.8855\nEpoch 19, Loss: 0.7200\nEpoch 20, Loss: 0.5919\nEpoch 21, Loss: 0.4826\nEpoch 22, Loss: 0.3885\nEpoch 23, Loss: 0.3050\nEpoch 24, Loss: 0.2457\nEpoch 25, Loss: 0.1900\nEpoch 26, Loss: 0.1506\nEpoch 27, Loss: 0.1308\nEpoch 28, Loss: 0.1163\nEpoch 29, Loss: 0.1014\nEpoch 30, Loss: 0.0935\nValidation Loss: 7.2940\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## Save Model","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), \"seq2seq_model.pth\")\n# tokenizer.save_pretrained(\"hf_tokenizer\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:35:19.714758Z","iopub.execute_input":"2025-05-15T11:35:19.715206Z","iopub.status.idle":"2025-05-15T11:35:21.115155Z","shell.execute_reply.started":"2025-05-15T11:35:19.715188Z","shell.execute_reply":"2025-05-15T11:35:21.114332Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def translate_sentence(sentence):\n    model.eval()\n    tokens = tokenizer.encode(sentence, return_tensors='pt')[0]\n    src_tensor = pad_sequence(tokens, max_ar_len).unsqueeze(0).to(device)\n    src_tensor = src_tensor.long()  # ensure long for embedding\n\n    with torch.no_grad():\n        encoder_outputs, (hidden, cell) = model.encoder(src_tensor)\n\n        # Use bos_token_id if available, else pad_token_id as start token\n        start_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\n        input = torch.tensor([start_token], device=device)\n\n        outputs = []\n        for _ in range(max_en_len):\n            output, hidden, cell = model.decoder(input, hidden, cell, encoder_outputs)\n            top1 = output.argmax(1).item()\n            if top1 == tokenizer.eos_token_id:\n                break\n            outputs.append(top1)\n            input = torch.tensor([top1], device=device)\n\n    return tokenizer.decode(outputs, skip_special_tokens=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:35:21.116033Z","iopub.execute_input":"2025-05-15T11:35:21.116300Z","iopub.status.idle":"2025-05-15T11:35:21.122250Z","shell.execute_reply.started":"2025-05-15T11:35:21.116277Z","shell.execute_reply":"2025-05-15T11:35:21.121304Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"print(\"\\nSample Translation:\")\nprint(\"AR:\", arabic_sentences[3])\nprint(\"EN:\", translate_sentence(arabic_sentences[3]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:35:50.887742Z","iopub.execute_input":"2025-05-15T11:35:50.888006Z","iopub.status.idle":"2025-05-15T11:35:50.940930Z","shell.execute_reply.started":"2025-05-15T11:35:50.887985Z","shell.execute_reply":"2025-05-15T11:35:50.940402Z"}},"outputs":[{"name":"stdout","text":"\nSample Translation:\nAR: استجمع توم ما يكفي من الشجاعة لطلب علاوة .\nEN: k summoned up enough courage to ask for a raise.\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}